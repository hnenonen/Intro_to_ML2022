---
title: "IML Project Report"
author: "Heikki Nenonen, Arttu Koskinen, Niko Petjakko"
date: "`r Sys.Date()`"
output:
  html_document: 
    df_print: paged
    theme: cerulean
    highlight: haddock
    code_fold: show
  pdf_document: default
---

## Todo

-   ~~dummy classifier~~

-   ~~class4 -\> event/nonevent, week1 exe?~~

-   ~~drop partlybad, pelkkää FALSEa~~

-   **varianssit mukana/ei mukana? ei one hot -\> yksinkertaistaa liikaa ja tarkoitettu kategoriseen dataan**

-   ~~date? paljon informaatiota, mutta halutaanko muuttujaksi \<- opeta 2000-2008, testaa 2009-2011 / kysy slack test_hidden ei - - date, jätetäänkö pois? good riddance!~~

-   ~~train, test, cv-10?~~

-   ~~itse logisticregression, week2 exe1 \<- lasso/ridge~~

-   ~~accuracy, perplexity, week2 exe1~~

-   accuracy of our accuracy? \<-- malli train+test, vähän parempi kuin pelkkä train?

-   ~~class4 -\> nonevent/1a/1b/II/ = 0,1,2,3~~

-   ~~try normalising data - google if needed for RF~~ needed for PCA

-   try PCA (not implemented yet)

-   multiclass classifier tod.näkösyydet eivät näy atm

    ```{r include=FALSE}
    library(reticulate)
    ```

    ### Read data

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

npf_test = pd.read_csv("initial_data/npf_test_hidden.csv")
npf_train = pd.read_csv("initial_data/npf_train.csv")


```

### Preprocess data

```{python echo=TRUE}

npf_train_test = npf_train.set_index("date")
npf_train_test = npf_train_test.drop(['id', 'partlybad'], axis=1)

class2 = np.array(["nonevent", "event"])
class2 = class2[(npf_train_test["class4"]!="nonevent").astype(int)]
#class2 = class2.apply(lambda x: 1 if "event" else 0)
npf_train_test.insert(loc=0, column="class2", value=class2)

npf_train_test["class2"].replace(["event", "nonevent"],[1,0], inplace=True)
npf_train_test["class4"].replace(["nonevent", "Ia", "Ib", "II"],[0, 1, 2, 3], inplace=True)

#DROPS STDS
npf_train_test = npf_train_test.filter(regex='mean|class4|class2')
```

Stds didn't seem to affect the accuracies, and removal of them lightens the computation

First five columns and rows

```{r}
knitr::kable(head(py$npf_train_test[,1:5]), row.names = TRUE, digits = 2)
```

### Imports

We chose to compare many of the models used in the course exercise sets

```{python}
from sklearn.dummy import DummyClassifier
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, accuracy_score

from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

from sklearn.neighbors import KNeighborsClassifier
```

### Functions for model loss & accuracy

```{python}
def loss(X_tr, y_tr, X_te, y_te, m):
    return mean_squared_error(y_te, m.fit(X_tr, y_tr).predict(X_te), squared=False)


def accuracy(X_tr, y_tr, X_te, y_te, m):
    return accuracy_score(y_te, m.fit(X_tr, y_tr).predict(X_te))

```

### Train, fit, evaluate

```{python}


def magic(models, classtype):
  
  X = npf_train_test.drop(["class2", "class4"], axis=1, inplace=False)
  y = npf_train_test[classtype]
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, train_size=0.8, random_state=41, shuffle=True, stratify=y
  )
  
  res = pd.DataFrame(index=models)
  # Loss on training data, for model trained on training data:
  res["train_loss"] = [loss(X_train, y_train, X_train, y_train, m) for m in models]
  # Cross-validation loss:
  res["cv_loss"] = [
      -cross_val_score(
          m, X_train, y_train, cv=10, scoring="neg_root_mean_squared_error"
      ).mean()
      for m in models
  ]
  # Los on test data, for model trained on training data:
  res["test_loss"] = [loss(X_train, y_train, X_test, y_test, m) for m in models]
  res["test_accuracy"] = [accuracy(X_train, y_train, X_test, y_test, m) for m in models]
  
  perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
  
  #temporary solution since svm is weird with perplexity, KEEP SVM LAST!!
  list = [perplexity(m.predict_proba(X_test)[:,1]) for m in models[0:-1]]
  res["test_perplexity"]= np.append(list, np.nan)
  
  return res

```

### Define and compare models

```{python include=FALSE}

models = [DummyClassifier(), 
  LogisticRegression(penalty="l2", C=1_000, solver="lbfgs"), 
  LogisticRegression(penalty="elasticnet", l1_ratio=0.5, solver="saga"), 
  LogisticRegression(penalty="l2", C=1, solver="saga"), 
  LogisticRegression(penalty="none", solver="lbfgs"), 
  LogisticRegression(penalty="l1", C=1, solver="saga"),
  GaussianNB(),
  QuadraticDiscriminantAnalysis(),
  RandomForestClassifier(),
  KNeighborsClassifier(),
  make_pipeline(StandardScaler(), SVC(gamma='auto'))
  ]
  
  
results_class2 = magic(models, 'class2')
results_class4 = magic(models, 'class4')
```

```{r warning=FALSE, message=FALSE}
results_class2 <- py$results_class2
rownames(results_class2) <- c('Dummy','Logistic1','Logistic2','Logistic3','Logistic4','Logistic5','GausNB','QDA', 'RF', 'KN', 'SVM')
results_class4 <- py$results_class4
rownames(results_class4) <- c('Dummy','Logistic1','Logistic2','Logistic3','Logistic4','Logistic5','GausNB','QDA', 'RF', 'KN', 'SVM')

library(kableExtra)
library(tidyverse)

knitr::kable(results_class2, row.names = TRUE, digits = 2)%>%kable_styling()%>%row_spec(9,bold=T,hline_after = T)
knitr::kable(results_class4, row.names = TRUE, digits = 2)%>%kable_styling()%>%row_spec(9,bold=T,hline_after = T)
```

### Classification with random forest (James et al.)

**Random forest is a combination of bagging (bootstrap + aggregation) multiple decision trees and decorrelating them to explore the model space more thoroughly. Sklearn's RandomForestClassifier inherently supports multiclass classification, so therefore we could straightforwardly use it for both of the classification problems.**

Decision tree is a binary tree that splits the training data, maximizing the quality at each split as measured by some ```criterion```, and ends up in leaf nodes, where only a single class is present. Bootstrap creates ```n_estimators``` new trees from randomly sampled rows of the training data set. At each split, decorrelation is achieved by sampling $m$ predictors for the splits to use. Finally, the predictions made by the trees are aggregated through majority vote to output the predicted class.

With a sufficient number of trees, the bagging prevents overfitting, but may still get stuck in local optima. The addition of decorrelating avoids this downside, reducing test and OOB (out-of-bag) errors.

### Compare different parameters for RandomForestClassifier: impurity measure and number of trees

insight: we only compared parameters for class4

Impurity measure determines how the decision trees are built in the random forest from the features in the dataset [book]. We noticed that gini gives the best accuracy, so we chose that.

number of trees=100

(default parameters for sklearn RandomForest)

```{python include=FALSE}
models = [  
  RandomForestClassifier(criterion='gini'),
  RandomForestClassifier(criterion='log_loss'),
  RandomForestClassifier(criterion='entropy'),
  RandomForestClassifier(criterion='gini', n_estimators=50),
  RandomForestClassifier(criterion='gini', n_estimators=250),
  RandomForestClassifier(criterion='log_loss')]
  
results_class4 = magic(models, 'class4')


```

```{r}
results_class4 <- py$results_class4
rownames(results_class4) <- c('gini', 'log_loss','entropy','gini n=50', 'gini n=250', 'useless')
knitr::kable(results_class4, row.names = TRUE, digits = 2)%>%kable_styling()%>%row_spec(1,bold=T,hline_after = T)

```

### Final model

We first train the model and classify events and nonevents (class2)

```{python results='hide'}
npf_test_clean = npf_test.drop(['id', 'partlybad', 'date'], axis=1)

#DROPS STDS
npf_test_clean = npf_test_clean.filter(regex='mean|class4|class2')

X = npf_train_test.drop(columns=['class2', 'class4'])
y = npf_train_test['class2']
rfc = RandomForestClassifier(criterion='gini')
model = rfc.fit(X, y)

predict_x = npf_test_clean.drop(columns='class4')

probas = pd.DataFrame(predict_x.copy())


def get_predict_proba(row, model):
  probas = model.predict_proba(row.values.reshape(1,-1))
  
  nonevent_p = probas[0][0]
  event_p = 1-nonevent_p
  
  return event_p 

probas['proba'] = probas.apply(lambda row: get_predict_proba(row, model), axis=1)

predicts = pd.DataFrame(model.predict(predict_x))

final = predicts.merge(probas['proba'].to_frame(), left_index=True, right_index=True)

final[0].replace([0, 1],["nonevent", "event"], inplace=True)
```

Then we train the multiclassifier using only rows that were classified as events

```{python results='hide'}

npf_test_clean = npf_test.drop(['id', 'partlybad', 'date'], axis=1)

# DROPS STDS
npf_test_clean = npf_test_clean.filter(regex='mean|class4|class2')

# SELECT ROWS WHERE BINARY CLASSIFIER PREDICTED EVENTS AS TRAINING DATA
npf_train_events = npf_train_test[npf_train_test['class2']==1]

X = npf_train_events.drop(columns=['class2', 'class4'])
y = npf_train_events['class4']
rfc = RandomForestClassifier(criterion='gini')
model = rfc.fit(X, y)

predict_x = npf_test_clean.loc[final[final[0]=='event'].index].drop(columns='class4')

predicts = pd.DataFrame(model.predict(predict_x))

predicts[0].replace([1, 2, 3],["Ia", "Ib", "II"], inplace=True)

i = 0
for index, row in final.iterrows():
  if row[0]=='event':
    final.at[index, 0] = predicts.iloc[i,0]
    i += 1
  
```

Save answers.csv

```{python eval=FALSE}
row0 = pd.DataFrame({0: 0.9, 'proba':''}, index =[0])
row1 = pd.DataFrame({0: 'class4', 'proba':'p'}, index =[0])
merged = pd.concat([row1, final])
merged = pd.concat([row0, merged])

merged.to_csv('answers.csv', index=False, header=False)
```

### Why the good accuracy?

Challenge set binary accuracy $\approx 88\%$, multi $\approx 71\%$

Although random forest bootstraps, we probably got lucky since randomness is involved

#### Why infinite perplexity?

Some points in the test set probably got assigned 0% probability (either outliers or then the model is not general enough?)

#### What would have we done different?

More EDA?

Would have tried PCA if had time (some tests suggested that it could have improved test accuracy)

## References

<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>

<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>

<https://scikit-learn.org/stable/modules/multiclass.html>

Course book
